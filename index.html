<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jie Hu</title>
  
  <meta name="author" content="Jie Hu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="imgs/panada.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:66%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jie Hu</name>
              </p>
              <p> I am a fourth-year Ph.D. candidate at <a href="https://en.xmu.edu.cn/" target="_blank">Xiamen University</a>, working under the supervision of Prof. <a href="https://mac.xmu.edu.cn/rrji_en/" target="_blank">Rongrong Ji</a>. My current research interests are in scene understanding and knowledge disentanglement. In June 2023, I will join <a href="https://www.catl.com/en/" target="_blank">CATL</a> as a researcher in industrial AI.
              </p>
              <p style="text-align:center">
                <a href="hujie.cpp@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=cyLKKD8AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/hujiecpp">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="imgs/HJ.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="imgs/HJ.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%;vertical-align:middle">
              <img style="width:100%;max-width:100%" src="imgs/CVPR23_YOSO.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_You_Only_Segment_Once_Towards_Real-Time_Panoptic_Segmentation_CVPR_2023_paper.pdf">
              <papertitle>You Only Segment Once: Towards Real-Time Panoptic Segmentation</papertitle>
              </a>
              <br>
              <strong>Jie Hu</strong>,
              <a href="https://linyanai.github.io/">Linyan Huang</a>,
              <a href="https://rentainhe.github.io/">Tianhe Ren</a>,
              <a href="https://scholar.google.com/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao">Shengchuan Zhang</a>, 
              <a href="https://scholar.google.com/citations?user=lRSD7PQAAAAJ&hl=zh-CN">Rongrong Ji</a>, 
              <a href="https://scholar.google.com/citations?user=iYEcVaAAAAAJ&hl=zh-CN&oi=ao">Liujuan Cao</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023</em>
              <br>
              <div class="paper" id="CVPR23_YOSO">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_You_Only_Segment_Once_Towards_Real-Time_Panoptic_Segmentation_CVPR_2023_paper.pdf">paper</a> /
                <a href="https://github.com/hujiecpp/YOSO">code</a> / 
                <a href="https://www.youtube.com/watch?v=wQCKerE_NmQ">video</a>
              </div>
              <p>In this paper, we show the complex task of panoptic segmentation can run over 30 FPS with competitive PQ performance, which is achieved by the proposed YOSO with a feature pyramid aggregator and a separable dynamic decoder.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%;vertical-align:middle">
              <img style="width:100%;max-width:100%" src="imgs/CVPR23_DistilPose.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_DistilPose_Tokenized_Pose_Regression_With_Heatmap_Distillation_CVPR_2023_paper.pdf">
              <papertitle>DistilPose: Tokenized Pose Regression with Heatmap Distillation</papertitle>
              </a>
              <br>
              <a href="">Suhang Ye<sup>*</sup></a>,
              <a href="">Yingyi Zhang<sup>*</sup></a>,
              <strong>Jie Hu<sup>*</sup></strong>,
              <a href="https://scholar.google.com/citations?user=iYEcVaAAAAAJ&hl=zh-CN&oi=ao">Liujuan Cao</a>,
              <a href="https://scholar.google.com/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao">Shengchuan Zhang</a>,
              <a href="">Lei Shen</a>,
              <a href="">Jun Wang</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=OGf40fkAAAAJ&view_op=list_works&sortby=pubdate">Shouhong Ding</a>,
              <a href="https://scholar.google.com/citations?user=lRSD7PQAAAAJ&hl=zh-CN">Rongrong Ji</a> (<sup>*</sup>Equal Contribution)
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023</em>
              <br>
              <div class="paper" id="CVPR23_DistilPose">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_DistilPose_Tokenized_Pose_Regression_With_Heatmap_Distillation_CVPR_2023_paper.pdf">paper</a> /
                <a href="https://github.com/yshMars/DistilPose">code</a>
              </div>
              <p>This paper proposes DistilPose that distills heatmap-based human pose estimation models into regression-based ones to speed up the pipeline.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%;vertical-align:middle">
              <img style="width:100%;max-width:100%" src="imgs/arXiv21_ISTR.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://arxiv.org/pdf/2105.00637.pdf">
              <papertitle>ISTR: End-to-end Instance Segmentation with Transformers</papertitle>
              </a>
              <br>
              <strong>Jie Hu</strong>,
              <a href="https://scholar.google.com/citations?user=iYEcVaAAAAAJ&hl=zh-CN&oi=ao">Liujuan Cao</a>,
              <a href="">Yao Lu</a>,
              <a href="https://scholar.google.com/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao">Shengchuan Zhang</a>,
              <a href="">Yan Wang</a>,
              <a href="http://keli.info/">Ke Li</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=gvCMrqsAAAAJ">Feiyue Huang</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=z84rLjoAAAAJ">Ling Shao</a>,
              <a href="https://scholar.google.com/citations?user=lRSD7PQAAAAJ&hl=zh-CN">Rongrong Ji</a>
              <br>
              <em>arXiv preprint arXiv:2105.00637, 2021</em>
              <br>
              <div class="paper" id="arXiv21_ISTR">
                <a href="https://arxiv.org/pdf/2105.00637.pdf">arxiv</a> /
                <a href="https://github.com/hujiecpp/ISTR">code</a>
              </div>
              <p>This paper proposes a transformer-based instance segmentation framework, which encodes masks into embeddings to regress them.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%;vertical-align:middle">
              <img style="width:100%;max-width:100%" src="imgs/ICCV21_NAD.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_Architecture_Disentanglement_for_Deep_Neural_Networks_ICCV_2021_paper.pdf">
              <papertitle>Architecture Disentanglement for Deep Neural Networks</papertitle>
              </a>
              <br>
              <strong>Jie Hu</strong>,
              <a href="https://scholar.google.com/citations?user=iYEcVaAAAAAJ&hl=zh-CN&oi=ao">Liujuan Cao</a>,
              <a href="https://scholar.google.com/citations?user=tjEfgsEAAAAJ&hl=zh-CN&oi=ao">Qixiang Ye</a>,
              <a href="">Tong Tong</a>,
              <a href="https://scholar.google.com/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao">Shengchuan Zhang</a>,
              <a href="http://keli.info/">Ke Li</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=gvCMrqsAAAAJ">Feiyue Huang</a>,
              <a href="https://scholar.google.com/citations?user=lRSD7PQAAAAJ&hl=zh-CN">Rongrong Ji</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=z84rLjoAAAAJ">Ling Shao</a>
              <br>
              <em>IEEE/CVF International Conference on Computer Vision (ICCV), oral, 2021</em>
              <br>
              <div class="paper" id="ICCV21_NAD">
                <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_Architecture_Disentanglement_for_Deep_Neural_Networks_ICCV_2021_paper.pdf">paper</a> /
                <a href="https://github.com/hujiecpp/NAD">code</a>
              </div>
              <p>This paper proposes to disentangle deep neural networks via information bottleneck to understand the inner workings of them.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%;vertical-align:middle">
              <img style="width:100%;max-width:100%" src="imgs/CVPR21_HiSD.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Image-to-Image_Translation_via_Hierarchical_Style_Disentanglement_CVPR_2021_paper.pdf">
              <papertitle>Image-to-image Translation via Hierarchical Style Disentanglement</papertitle>
              </a>
              <br>
              <a href="https://imlixinyang.github.io/resume-cv/">Xinyang Li</a>,
              <a href="https://scholar.google.com/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao">Shengchuan Zhang</a>,
              <strong>Jie Hu</strong>,
              <a href="https://scholar.google.com/citations?user=iYEcVaAAAAAJ&hl=zh-CN&oi=ao">Liujuan Cao</a>,
              <a href="https://scholar.google.com/citations?user=x3X-qysAAAAJ&hl=zh-CN">Xiaopeng Hong</a>,
              <a href="https://scholar.google.com/citations?user=9PLaP2cAAAAJ&hl=en">Xudong Mao</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=gvCMrqsAAAAJ">Feiyue Huang</a>,
              <a href="">Yongjian Wu</a>,
              <a href="https://scholar.google.com/citations?user=lRSD7PQAAAAJ&hl=zh-CN">Rongrong Ji</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), oral, 2021</em>
              <br>
              <div class="paper" id="CVPR21_HiSD">
                <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Image-to-Image_Translation_via_Hierarchical_Style_Disentanglement_CVPR_2021_paper.pdf">paper</a> /
                <a href="https://github.com/imlixinyang/HiSD">code</a>
              </div>
              <p>This paper presents a novel method for image-to-image translation that uses a hierarchical tree to re-arrange the image attributes and achieve style disentanglement.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%;vertical-align:middle">
              <img style="width:100%;max-width:100%" src="imgs/NeurIPS19_ICP.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://proceedings.neurips.cc/paper/2019/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf">
              <papertitle>Information Competing Process for Learning Diversified Representations</papertitle>
              </a>
              <br>
              <strong>Jie Hu</strong>,
              <a href="https://scholar.google.com/citations?user=lRSD7PQAAAAJ&hl=zh-CN">Rongrong Ji</a>,
              <a href="https://scholar.google.com/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao">Shengchuan Zhang</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=KPMK3B4AAAAJ">Xiaoshuai Sun</a>,
              <a href="https://scholar.google.com/citations?user=tjEfgsEAAAAJ&hl=zh-CN&oi=ao">Qixiang Ye</a>,

              <a href="https://scholar.google.com/citations?hl=en&user=fXN3dl0AAAAJ">Chia-Wen Lin</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=61b6eYkAAAAJ">Qi Tian</a>
              <br>
              <em>Neural Information Processing Systems (NeurIPS), 2019</em>
              <br>
              <div class="paper" id="NeurIPS19_ICP">
                <a href="https://proceedings.neurips.cc/paper/2019/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf">paper</a> /
                <a href="https://github.com/hujiecpp/InformationCompetingProcess/">code</a>
              </div>
              <p>This paper proposes a new approach that separates a representation into two parts with different mutual information constraints.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%;vertical-align:middle">
              <img style="width:100%;max-width:100%" src="imgs/CVPR19_VFT.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Hu_Towards_Visual_Feature_Translation_CVPR_2019_paper.pdf">
              <papertitle>Towards Visual Feature Translation</papertitle>
              </a>
              <br>
              <strong>Jie Hu</strong>,
              <a href="https://scholar.google.com/citations?user=lRSD7PQAAAAJ&hl=zh-CN">Rongrong Ji</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=BC7N2dYAAAAJ">Hong Liu</a>,
              <a href="https://scholar.google.com/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao">Shengchuan Zhang</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=OROjmc8AAAAJ">Cheng Deng</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=61b6eYkAAAAJ">Qi Tian</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019</em>
              <br>
              <div class="paper" id="CVPR19_VFT">
                <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Hu_Towards_Visual_Feature_Translation_CVPR_2019_paper.pdf">paper</a> /
                <a href="https://github.com/hujiecpp/VisualFeatureTranslation">code</a>
              </div>
              <p>This paper proposes to break through the barrier of using features across different visual search systems.</p>
            </td>
          </tr>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Template</a>
              </p>
            </td>
          </tr>
        </tbody></table> -->
      </td>
    </tr>
  </table>
</body>

</html>
