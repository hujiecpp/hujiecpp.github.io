<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jie Hu</title>
  
  <meta name="author" content="Jie Hu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="imgs/panada.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:66%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jie Hu</name>
              </p>
              <p> I currently serve as a research fellow at the <a href="https://www.nus.edu.sg" target="_blank">National University of Singapore</a>, where I collaborate with Professor <a href="https://sites.google.com/site/sitexinchaowang/" target="_blank">Xinchao Wang</a>. Previously, I worked as an engineer and postdoctoral researcher at <a href="https://www.catl.com/en/" target="_blank">Contemporary Amperex Technology Co. Limited</a>, under the guidance of Professor <a href="https://me.engin.umich.edu/people/faculty/jun-ni/" target="_blank">Jun Ni</a>. I earned my Ph.D. degree from <a href="https://en.xmu.edu.cn/" target="_blank">Xiamen University</a>, with my research supervised by Professor <a href="https://mac.xmu.edu.cn/rrji_en/" target="_blank">Rongrong Ji</a>. My research interests lie in the fields of 2D-3D representation learning, scene understanding, and generation.
              </p>
              <p style="text-align:center">
                <a href="mailto:hujie.cpp@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/JieResume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=cyLKKD8AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/hujiecpp">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="imgs/HJ.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="imgs/HJ.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
          <td style="padding:20px;width:30%;max-width:30%;vertical-align:middle">
            <img style="width:100%;max-width:100%" src="imgs/arXiv25_PE3R.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <a href="https://arxiv.org/pdf/2503.07507">
            <papertitle>PE3R: Perception-Efficient 3D Reconstruction</papertitle>
            </a>
            <br>
            <strong>Jie Hu</strong>,
            <a href="https://littlepure2333.github.io/home/">Shizun Wang</a>,
            <a href="https://sites.google.com/site/sitexinchaowang/">Xinchao Wang</a>,
            <br>
            <em>arXiv, 2025</em>
            <br>
            <div class="paper" id="arXiv25_PE3R">
              <a href="https://arxiv.org/pdf/2503.07507">arxiv version</a> /
              <a href="https://github.com/hujiecpp/PE3R">code</a> /
              <a href="https://youtu.be/iFRijE4GQv4">video</a> /
              <a href="https://huggingface.co/spaces/hujiecpp/PE3R">demo</a>
            </div>
            <p>PE3R reconstructs 3D scenes using only 2D images and enables semantic understanding through language.</p>
          </td>
        </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%;vertical-align:middle">
              <img style="width:100%;max-width:100%" src="imgs/TIP24_ISTR.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://ieeexplore.ieee.org/abstract/document/10499209">
              <papertitle>ISTR: Mask-Embedding-Based Instance Segmentation Transformer</papertitle>
              </a>
              <br>
              <strong>Jie Hu</strong>,
              <a href="">Yao Lu</a>,
              <a href="https://scholar.google.com/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao">Shengchuan Zhang</a>,
              <a href="https://scholar.google.com/citations?user=iYEcVaAAAAAJ&hl=zh-CN&oi=ao">Liujuan Cao</a>,
              <br>
              <em>IEEE Transactions on Image Processing (TIP), 2024</em>
              <br>
              <div class="paper" id="TIP24_ISTR">
                <a href="https://ieeexplore.ieee.org/abstract/document/10499209">journal version</a> /
                <a href="https://arxiv.org/pdf/2105.00637.pdf">arxiv version</a> /
                <a href="https://github.com/hujiecpp/ISTR">code</a>
              </div>
              <p>This paper proposes a transformer-based instance segmentation framework, which encodes masks into embeddings to regress them.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%;vertical-align:middle">
              <img style="width:100%;max-width:100%" src="imgs/ICCV23_PAIS.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="">
              <papertitle>Pseudo-label Alignment for Semi-supervised Instance Segmentation</papertitle>
              </a>
              <br>
              <strong>Jie Hu<sup>*</sup></strong>,
              <a href="">Chen Chen<sup>*</sup></a>,
              <a href="https://scholar.google.com/citations?user=iYEcVaAAAAAJ&hl=zh-CN&oi=ao">Liujuan Cao</a>
              <a href="https://scholar.google.com/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao">Shengchuan Zhang</a>,
              <a href="">Annan Shu</a>,
              <a href="https://scholar.google.com.au/citations?user=yw-rcj4AAAAJ&hl=en">Guannan Jiang</a>,
              <a href="https://scholar.google.com/citations?user=lRSD7PQAAAAJ&hl=zh-CN">Rongrong Ji</a> (<sup>*</sup>Equal Contribution)
              <br>
              <em>IEEE/CVF International Conference on Computer Vision (ICCV), 2023</em>
              <br>
              <div class="paper" id="ICCV23_PAIS">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_Pseudo-label_Alignment_for_Semi-supervised_Instance_Segmentation_ICCV_2023_paper.pdf">paper</a> /
                <a href="https://github.com/hujiecpp/PAIS">code</a>
              </div>
              <p>This paper proposes a novel framework, PAIS, that aligns the pseudo-labels of unannotated images with varying class and mask quality for semi-supervised instance segmentation, achieving state-of-the-art results on COCO and Cityscapes datasets.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%;vertical-align:middle">
              <img style="width:100%;max-width:100%" src="imgs/CVPR23_YOSO.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_You_Only_Segment_Once_Towards_Real-Time_Panoptic_Segmentation_CVPR_2023_paper.pdf">
              <papertitle>You Only Segment Once: Towards Real-Time Panoptic Segmentation</papertitle>
              </a>
              <br>
              <strong>Jie Hu</strong>,
              <a href="https://linyanai.github.io/">Linyan Huang</a>,
              <a href="https://rentainhe.github.io/">Tianhe Ren</a>,
              <a href="https://scholar.google.com/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao">Shengchuan Zhang</a>, 
              <a href="https://scholar.google.com/citations?user=lRSD7PQAAAAJ&hl=zh-CN">Rongrong Ji</a>, 
              <a href="https://scholar.google.com/citations?user=iYEcVaAAAAAJ&hl=zh-CN&oi=ao">Liujuan Cao</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023</em>
              <br>
              <div class="paper" id="CVPR23_YOSO">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_You_Only_Segment_Once_Towards_Real-Time_Panoptic_Segmentation_CVPR_2023_paper.pdf">paper</a> /
                <a href="https://github.com/hujiecpp/YOSO">code</a> / 
                <a href="https://www.youtube.com/watch?v=wQCKerE_NmQ">video</a>
              </div>
              <p>In this paper, we show the complex task of panoptic segmentation can run over 30 FPS with competitive PQ performance, which is achieved by the proposed YOSO with a feature pyramid aggregator and a separable dynamic decoder.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%;vertical-align:middle">
              <img style="width:100%;max-width:100%" src="imgs/CVPR23_DistilPose.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_DistilPose_Tokenized_Pose_Regression_With_Heatmap_Distillation_CVPR_2023_paper.pdf">
              <papertitle>DistilPose: Tokenized Pose Regression with Heatmap Distillation</papertitle>
              </a>
              <br>
              <a href="">Suhang Ye<sup>*</sup></a>,
              <a href="">Yingyi Zhang<sup>*</sup></a>,
              <strong>Jie Hu<sup>*</sup></strong>,
              <a href="https://scholar.google.com/citations?user=iYEcVaAAAAAJ&hl=zh-CN&oi=ao">Liujuan Cao</a>,
              <a href="https://scholar.google.com/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao">Shengchuan Zhang</a>,
              <a href="">Lei Shen</a>,
              <a href="">Jun Wang</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=OGf40fkAAAAJ&view_op=list_works&sortby=pubdate">Shouhong Ding</a>,
              <a href="https://scholar.google.com/citations?user=lRSD7PQAAAAJ&hl=zh-CN">Rongrong Ji</a> (<sup>*</sup>Equal Contribution)
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023</em>
              <br>
              <div class="paper" id="CVPR23_DistilPose">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_DistilPose_Tokenized_Pose_Regression_With_Heatmap_Distillation_CVPR_2023_paper.pdf">paper</a> /
                <a href="https://github.com/yshMars/DistilPose">code</a>
              </div>
              <p>This paper proposes DistilPose that distills heatmap-based human pose estimation models into regression-based ones to speed up the pipeline.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%;vertical-align:middle">
              <img style="width:100%;max-width:100%" src="imgs/ICCV21_NAD.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_Architecture_Disentanglement_for_Deep_Neural_Networks_ICCV_2021_paper.pdf">
              <papertitle>Architecture Disentanglement for Deep Neural Networks</papertitle>
              </a>
              <br>
              <strong>Jie Hu</strong>,
              <a href="https://scholar.google.com/citations?user=iYEcVaAAAAAJ&hl=zh-CN&oi=ao">Liujuan Cao</a>,
              <a href="https://scholar.google.com/citations?user=tjEfgsEAAAAJ&hl=zh-CN&oi=ao">Qixiang Ye</a>,
              <a href="">Tong Tong</a>,
              <a href="https://scholar.google.com/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao">Shengchuan Zhang</a>,
              <a href="http://keli.info/">Ke Li</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=gvCMrqsAAAAJ">Feiyue Huang</a>,
              <a href="https://scholar.google.com/citations?user=lRSD7PQAAAAJ&hl=zh-CN">Rongrong Ji</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=z84rLjoAAAAJ">Ling Shao</a>
              <br>
              <em>IEEE/CVF International Conference on Computer Vision (ICCV), oral, 2021</em>
              <br>
              <div class="paper" id="ICCV21_NAD">
                <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_Architecture_Disentanglement_for_Deep_Neural_Networks_ICCV_2021_paper.pdf">paper</a> /
                <a href="https://github.com/hujiecpp/NAD">code</a>
              </div>
              <p>This paper proposes to disentangle deep neural networks via information bottleneck to understand the inner workings of them.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%;vertical-align:middle">
              <img style="width:100%;max-width:100%" src="imgs/CVPR21_HiSD.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Image-to-Image_Translation_via_Hierarchical_Style_Disentanglement_CVPR_2021_paper.pdf">
              <papertitle>Image-to-image Translation via Hierarchical Style Disentanglement</papertitle>
              </a>
              <br>
              <a href="https://imlixinyang.github.io/resume-cv/">Xinyang Li</a>,
              <a href="https://scholar.google.com/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao">Shengchuan Zhang</a>,
              <strong>Jie Hu</strong>,
              <a href="https://scholar.google.com/citations?user=iYEcVaAAAAAJ&hl=zh-CN&oi=ao">Liujuan Cao</a>,
              <a href="https://scholar.google.com/citations?user=x3X-qysAAAAJ&hl=zh-CN">Xiaopeng Hong</a>,
              <a href="https://scholar.google.com/citations?user=9PLaP2cAAAAJ&hl=en">Xudong Mao</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=gvCMrqsAAAAJ">Feiyue Huang</a>,
              <a href="">Yongjian Wu</a>,
              <a href="https://scholar.google.com/citations?user=lRSD7PQAAAAJ&hl=zh-CN">Rongrong Ji</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), oral, 2021</em>
              <br>
              <div class="paper" id="CVPR21_HiSD">
                <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Image-to-Image_Translation_via_Hierarchical_Style_Disentanglement_CVPR_2021_paper.pdf">paper</a> /
                <a href="https://github.com/imlixinyang/HiSD">code</a>
              </div>
              <p>This paper presents a novel method for image-to-image translation that uses a hierarchical tree to re-arrange the image attributes and achieve style disentanglement.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%;vertical-align:middle">
              <img style="width:100%;max-width:100%" src="imgs/NeurIPS19_ICP.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://proceedings.neurips.cc/paper/2019/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf">
              <papertitle>Information Competing Process for Learning Diversified Representations</papertitle>
              </a>
              <br>
              <strong>Jie Hu</strong>,
              <a href="https://scholar.google.com/citations?user=lRSD7PQAAAAJ&hl=zh-CN">Rongrong Ji</a>,
              <a href="https://scholar.google.com/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao">Shengchuan Zhang</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=KPMK3B4AAAAJ">Xiaoshuai Sun</a>,
              <a href="https://scholar.google.com/citations?user=tjEfgsEAAAAJ&hl=zh-CN&oi=ao">Qixiang Ye</a>,

              <a href="https://scholar.google.com/citations?hl=en&user=fXN3dl0AAAAJ">Chia-Wen Lin</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=61b6eYkAAAAJ">Qi Tian</a>
              <br>
              <em>Neural Information Processing Systems (NeurIPS), 2019</em>
              <br>
              <div class="paper" id="NeurIPS19_ICP">
                <a href="https://proceedings.neurips.cc/paper/2019/file/71f6278d140af599e06ad9bf1ba03cb0-Paper.pdf">paper</a> /
                <a href="https://github.com/hujiecpp/InformationCompetingProcess/">code</a>
              </div>
              <p>This paper proposes a new approach that separates a representation into two parts with different mutual information constraints.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%;vertical-align:middle">
              <img style="width:100%;max-width:100%" src="imgs/CVPR19_VFT.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Hu_Towards_Visual_Feature_Translation_CVPR_2019_paper.pdf">
              <papertitle>Towards Visual Feature Translation</papertitle>
              </a>
              <br>
              <strong>Jie Hu</strong>,
              <a href="https://scholar.google.com/citations?user=lRSD7PQAAAAJ&hl=zh-CN">Rongrong Ji</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=BC7N2dYAAAAJ">Hong Liu</a>,
              <a href="https://scholar.google.com/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao">Shengchuan Zhang</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=OROjmc8AAAAJ">Cheng Deng</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=61b6eYkAAAAJ">Qi Tian</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019</em>
              <br>
              <div class="paper" id="CVPR19_VFT">
                <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Hu_Towards_Visual_Feature_Translation_CVPR_2019_paper.pdf">paper</a> /
                <a href="https://github.com/hujiecpp/VisualFeatureTranslation">code</a>
              </div>
              <p>This paper proposes to break through the barrier of using features across different visual search systems.</p>
            </td>
          </tr>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Template</a>
              </p>
            </td>
          </tr>
        </tbody></table> -->
      </td>
    </tr>
  </table>
</body>

</html>
